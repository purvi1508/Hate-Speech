{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b99760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\omen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to scrape from Twitter or Reddit?\n",
      " [Press 1 for Twitter, Press 2 for Reddit]\n",
      ":1\n",
      "Do you want to search for a particular user or a topic?\n",
      " [Press 1 for User, Press 2 for Topic]\n",
      ": 2\n",
      "Enter the number of tweets you want to scrape: 1000\n",
      "Enter the query: asian hate crime\n",
      "Do you want to scrape tweets sorted by top or latest?\n",
      " [Press 1 for Top, Press anything else for Latest]\n",
      ":1\n"
     ]
    }
   ],
   "source": [
    "# TOPIC: SCRAPE DATA FROM TWEET AND REDDIT AND PRE-PROCESS THEM\n",
    "# LAST UPDATED ON: 27/09/2022\n",
    "# TESTED ON: MacOS Monterey (12.6) [M1 Architecture] - Python 3.9.13  \n",
    "\n",
    "###################################\n",
    "########## REQUIREMENTS: ##########\n",
    "###################################\n",
    "# This code requires python 3.8+ to run, otherwise snscrape (Scrapping Module) doesn't work for reddit.\n",
    "# Please make sure the following libraries are installed: snscrape, pandas, nltk, contractions, emoji, pandas, re, string using pip or similar installers.\n",
    "\n",
    "##############################################\n",
    "########## SUPPORTED FUNCTIONALITY: ##########\n",
    "##############################################\n",
    "# This code scrapes data from either Twitter or Reddit and preprocesses the extracted text.\n",
    "\n",
    "########### FOR TWITTER: ##########\n",
    "# This code can scrape data from a particular User, or any searches.\n",
    "# Incase, of searches either the tweets can be extracted either in latest or top order.\n",
    "# Scrapped data are the tweets. The follwoing things are extracted:\n",
    "# ['Unique ID', 'Date', 'User', 'Tweet', 'Preproccesed Tweet']\n",
    "\n",
    "########## FOR REDDIT: ##########\n",
    "# This code can scrape data from a particular User, Sub-reddit or any searches.\n",
    "# Scraped data can include either comments or posts.\n",
    "# For Comments/Posts the following datas are extracted:\n",
    "# ['Unique ID', 'Date', 'Sub-reddit', 'Author', 'Title/Comment', 'Preprocessed Title/Comment']\n",
    "# Incase of Posts Title is extracted, and for Comments the Comment(body) is extracted.\n",
    "\n",
    "########## PRE-PROCESSING THE TEXT IS SAME FOR BOTH REDDIT AND TWITTER ##########\n",
    "# After Scrapping the Text Preprocessing the following preprocessing is done (these can be varied according to the use-case):\n",
    "# remove_urls(text): Removes the URLs in the text\n",
    "# expand_contractions(text): Expands the sentence say I'll go to I will go. This helps in the follwoing pre-processing of removing stop words etc.\n",
    "# text_lowercase(text): Converts all the words to lower-case, might be ignored in cases where CAPS highlights something, like say CAPS is usually associated with Shouting\n",
    "# remove_punctuation(text): Removes the Punctuations in the text\n",
    "# remove_numbers(text): Removes the numbers in the text, this is used for sentiment analysis\n",
    "# remove_extra_whitespace(text): Removes the extra white spaces in the sentence, helpful for easy tokenisation.\n",
    "# remove_stopwords(text): Removes unimportant words like is, are, am etc. But this might be ingored for context analysis.\n",
    "# replace_emojis_with_words(text): Replaces emoji with appropriate word (Essential for Sentiment Analysis).\n",
    "# lemmatize_word(text): Lemmantizes the sentences, and the final output is tokenized.\n",
    "\n",
    "\n",
    "# Importing the Modules:\n",
    "# For Text Extraction\n",
    "import snscrape.modules.reddit as snreddit # Installation: pip install snscrape --upgrade (Requires Python 3.8+ to run)\n",
    "import snscrape.modules.twitter as sntwitter # Installation: pip install snscrape --upgrade (Requires Python 3.8+ to run)\n",
    "import pandas as pd  # Installation: pip install pandas --upgrade\n",
    "\n",
    "\n",
    "# For Text Preprocessing\n",
    "import nltk  # Installation: pip install nltk --upgrade\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import emoji  # Installation: pip install emoji --upgrade\n",
    "import contractions  # Installation: pip install contractions --upgrade\n",
    "import re # Comes with the python 3.9 (for other versions check and install if not found)\n",
    "import string # Comes with the python 3.9 (for other versions check and install if not found)\n",
    "\n",
    "\n",
    "# For writing to the CSV File\n",
    "import csv # Comes with the python 3.9 (for other versions check and install if not found)\n",
    "##############################\n",
    "########## SCRAPING ##########\n",
    "##############################\n",
    "\n",
    "########## TWITTER: ##########\n",
    "\n",
    "# Used when tweets by a particular user needs to be extracted.\n",
    "def twitterUser(query, sizeOfQuery):\n",
    "    tweets = []\n",
    "    for tweet in sntwitter.TwitterUserScraper(query).get_items():\n",
    "        if len(tweets) == sizeOfQuery:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.id, tweet.date, tweet.user.username, tweet.content])\n",
    "    return (tweets)\n",
    "\n",
    "# Used when tweets of particular search topic needs to be extracted.\n",
    "def twitterTopic(query, sizeOfQuery):\n",
    "    tweets = []\n",
    "    topOrLatest = int(input(\"Do you want to scrape tweets sorted by top or latest?\\n [Press 1 for Top, Press anything else for Latest]\\n:\"))\n",
    "    flag = False\n",
    "    if topOrLatest == 1:\n",
    "        flag = True\n",
    "        # Basically the third attribute states if we need to scrape tweets sorted by top, i.e., if true we scrape by top.\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query, None, flag).get_items():\n",
    "        if len(tweets) == sizeOfQuery:\n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.id, tweet.date, tweet.user.username, tweet.content])\n",
    "    return (tweets)\n",
    "########## REDDIT: ##########\n",
    "# Used when Comments/Posts by a particular user is neened for scrapping.\n",
    "\n",
    "\n",
    "def redditUser(query, sizeOfQuery):\n",
    "    results = []\n",
    "    commentOrPost = int(input(\"Do you want to scrape for comments or posts?\\n [Press 1 for Comment, Press anything else for Posts]\\n: \"))\n",
    "    if commentOrPost == 1:\n",
    "           # Attributes of RedditUserScraper: (name, submissions: bool = True, comments: bool = True, before: Any | None = None, after: Any | None = None, **kwargs: Any) -> None\n",
    "        for result in snreddit.RedditUserScraper(query, False, True).get_items():\n",
    "            if len(results) == sizeOfQuery:\n",
    "                break\n",
    "            else:\n",
    "                results.append([result.id, result.created, result.subreddit, result.author, result.body])\n",
    "    else:\n",
    "        # Attributes of RedditUserScraper: (name, submissions: bool = True, comments: bool = True, before: Any | None = None, after: Any | None = None, **kwargs: Any) -> None\n",
    "        for result in snreddit.RedditUserScraper(query, True, False).get_items():\n",
    "            if len(results) == sizeOfQuery:\n",
    "                break\n",
    "            else:\n",
    "                results.append([result.id, result.created, result.subreddit, result.author, result.title])\n",
    "    return (results)\n",
    "\n",
    "# Used when Comments/Posts in a particular Sub-reddit is neened for scrapping.\n",
    "def redditSubreddit(query, sizeOfQuery):\n",
    "    results = []\n",
    "    commentOrPost = int(input(\"Do you want to scrape for comments or posts?\\n [Press 1 for Comment, Press anything else for Posts]\\n: \"))\n",
    "    if commentOrPost == 1:\n",
    "        # Attributes of RedditSubredditScraper: (name, submissions: bool = True, comments: bool = True, before: Any | None = None, after: Any | None = None, **kwargs: Any) -> None\n",
    "        for result in snreddit.RedditSubredditScraper(query, False, True).get_items():\n",
    "            if len(results) == sizeOfQuery:\n",
    "                break\n",
    "            else:\n",
    "                results.append([result.id, result.created, result.subreddit, result.author, result.body])\n",
    "    else:\n",
    "        # Attributes of RedditSubredditScraper: (name, submissions: bool = True, comments: bool = True, before: Any | None = None, after: Any | None = None, **kwargs: Any) -> None\n",
    "        for result in snreddit.RedditSubredditScraper(query, True, False).get_items():\n",
    "            if len(results) == sizeOfQuery:\n",
    "                break\n",
    "            else:\n",
    "                results.append([result.id, result.created, result.subreddit, result.author, result.title])\n",
    "    return (results)\n",
    "\n",
    "# Used when Comments/Posts of a particular search is neened for scrapping.\n",
    "def redditTopic(query, sizeOfQuery):\n",
    "    results = []\n",
    "    commentOrPost = int(input(\"Do you want to scrape for comments or posts?\\n [Press 1 for Comment, Press anything else for Posts]\\n: \"))\n",
    "    if commentOrPost == 1:\n",
    "        # Attributes of RedditSearchScraper: (name, submissions: bool = True, comments: bool = True, before: Any | None = None, after: Any | None = None, **kwargs: Any) -> None\n",
    "        for result in snreddit.RedditSearchScraper(query, False, True).get_items():\n",
    "            if len(results) == sizeOfQuery:\n",
    "                break\n",
    "            else:\n",
    "                results.append([result.id, result.created, result.subreddit, result.author, result.body])\n",
    "    else:\n",
    "        # Attributes of RedditSearchScraper: (name, submissions: bool = True, comments: bool = True, before: Any | None = None, after: Any | None = None, **kwargs: Any) -> None\n",
    "        for result in snreddit.RedditSearchScraper(query, True, False).get_items():\n",
    "            if len(results) == sizeOfQuery:\n",
    "                break\n",
    "            else:\n",
    "                results.append([result.id, result.created, result.subreddit, result.author, result.title])\n",
    "    return (results)\n",
    "\n",
    "# Doesn't Execute further unless a valid input is provided.\n",
    "twitterOrReddit = 0\n",
    "invalid = True\n",
    "while invalid == True:\n",
    "    twitterOrReddit = int(input(\"Do you want to scrape from Twitter or Reddit?\\n [Press 1 for Twitter, Press 2 for Reddit]\\n:\"))\n",
    "    if twitterOrReddit == 1 or twitterOrReddit == 2:\n",
    "        invalid = False\n",
    "    else:\n",
    "        print(\"Choice is invalid, select 1 or 2\")\n",
    "\n",
    "if twitterOrReddit == 1:\n",
    "\n",
    "    # Doesn't Execute further unless a valid input is provided.\n",
    "    choice = 0\n",
    "    invalid = True\n",
    "    while invalid == True:\n",
    "        choice = int(input(\"Do you want to search for a particular user or a topic?\\n [Press 1 for User, Press 2 for Topic]\\n: \"))\n",
    "        if choice == 1 or choice == 2:\n",
    "            invalid = False\n",
    "        else:\n",
    "            print(\"Choice is invalid, select 1 or 2\")\n",
    "\n",
    "    sizeOfQuery = int(input(\"Enter the number of tweets you want to scrape: \"))\n",
    "    query = input(\"Enter the query: \")\n",
    "    tweets = []\n",
    "    if choice == 1:\n",
    "        tweets = twitterUser(query, sizeOfQuery)\n",
    "    else:\n",
    "        tweets = twitterTopic(query, sizeOfQuery)\n",
    "else:\n",
    "\n",
    "    # Doesn't Execute further unless a valid input is provided.\n",
    "    choice = 0\n",
    "    invalid = True\n",
    "    while invalid == True:\n",
    "        choice = int(input(\"Do you want to search for a particular user, subreddit or a topic?\\n [Press 1 for User, Press 2 for Subreddit, Press 3 for Topic]\\n: \"))\n",
    "        if choice == 1 or choice == 2 or choice == 3:\n",
    "            invalid = False\n",
    "        else:\n",
    "            print(\"Choice is invalid, select 1, 2 or 3\")\n",
    "\n",
    "    sizeOfQuery = int(input(\"Enter the number of results you want to scrape: \"))\n",
    "    query = input(\"Enter the query: \")\n",
    "    results = []\n",
    "    if choice == 1:\n",
    "        results = redditUser(query, sizeOfQuery)\n",
    "    elif choice == 2:\n",
    "        results = redditSubreddit(query, sizeOfQuery)\n",
    "    else:\n",
    "        results = redditTopic(query, sizeOfQuery)\n",
    "\n",
    "# Can be used to check if the data has been succesfully scraped.\n",
    "# df = pd.DataFrame(tweets, columns=['Unique ID', 'Date', 'Sub-reddit', 'Author', 'Title/Comment'])\n",
    "# print(df)\n",
    "\n",
    "########################################\n",
    "########## TEXT PREPROCESSING ##########\n",
    "########################################\n",
    "\n",
    "# Remove URLs from text\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# expand contractions, Like I'll go will be converted to I will got\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Converts all the text into lower case, so basically HeLp, HELP, help all are same.\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Removes punctuation\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "# remove extra whitespaces\n",
    "def remove_extra_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# remove stopwords function\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Replace Emoji with words\n",
    "def replace_emojis_with_words(text):\n",
    "    return emoji.demojize(text, delimiters=(\"\", \"\"))\n",
    "\n",
    "# lemmatize string\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # provide context i.e. part-of-speech\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    text = remove_urls(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_extra_whitespace(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = replace_emojis_with_words(text)  # More useful for sentiment analysis\n",
    "    text = lemmatize_word(text)  # More useful for sentiment analysis\n",
    "    return text\n",
    "\n",
    "final_result = []\n",
    "\n",
    "# TWITTER:\n",
    "if (twitterOrReddit == 1):\n",
    "    for entry in tweets:\n",
    "        preprocessed_text = text_preprocessing(entry[3])\n",
    "        final_result.append([entry[0], entry[1], entry[2], entry[3], preprocessed_text])\n",
    "# REDDIT:\n",
    "else:\n",
    "    for entry in results:\n",
    "        preprocessed_text = text_preprocessing(entry[4])\n",
    "        final_result.append([entry[0], entry[1], entry[2], entry[3], entry[4], preprocessed_text])\n",
    "    df = pd.DataFrame(final_result, columns=['Unique ID', 'Date', 'Sub-reddit', 'Author', 'Title/Comment', 'Preprocessed Title/Comment'])\n",
    "    print(df)\n",
    "\n",
    "\n",
    "#######################################\n",
    "########## WRITE TO CSV FILE ##########\n",
    "#######################################\n",
    "\n",
    "# TWITTER:\n",
    "Details = ['Unique ID', 'Date', 'User', 'Tweet', 'Preproccesed Tweet']\n",
    "\n",
    "# REDDIT:\n",
    "if (twitterOrReddit == 2):\n",
    "    Details = ['Unique ID', 'Date', 'Sub-reddit', 'Author', 'Title/Comment', 'Preprocessed Title/Comment']\n",
    "\n",
    "with open('data1.csv', 'w', encoding='UTF8') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(Details)\n",
    "    write.writerows(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498d61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
